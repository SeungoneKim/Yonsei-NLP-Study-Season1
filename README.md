# Yonsei-NLP-Study-Season1
![main](./img/summary.png)
**[About Yonsei NLP Study]**
Yonsei NLP Study is consisted of 4 Students who are very passionate in the field of Natural Language Processing! <br>
Every week, each student reads 4 papers, and each student gets to give a presentation on 1 paper. <br>
After the presentation, all the students discuss about that paper and share ideas. <br>
**[About Season 1]**
Yonsei NLP Study Season 1 (2021.07.07 ~ 2021.08.25) <br>
Main Topic : Various Pretraining Methods of Language Models in NLP / Transformer based Models <br>
During this period we have covered 37 papers in total!
**[About this repository]**
This repository contains presentation materials, links to presentation videos, and a summary of all the papers we have studied in Yonsei NLP Study Season1(2021.07.07~2021.08.25). <br>
<br>
## About the Members
* [Seungone Kim(김승원)](https://github.com/SeungoneKim)
* [GuiJin Son(손규진)](https://github.com/ampehta)
* [Hyungjoo Chae(채형주)](https://github.com/kyle8581)
* [Sejun Joo(주세준)](https://github.com/joocjun)



## List of Papers we covered
```
1. Universal Language Model Fine-tuning for Text Classification [Link](https://arxiv.org/abs/1801.06146)
```
