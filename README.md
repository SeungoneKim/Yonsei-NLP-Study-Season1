# Yonsei-NLP-Study-Season1
![main](./img/summary.PNG) <br>
**[About Yonsei NLP Study]** <br>
Yonsei NLP Study is consisted of 4 Students who are very passionate in the field of Natural Language Processing! <br>
Every week, each student reads 4 papers, and each student gets to give a presentation on 1 paper. <br>
After the presentation, all the students discuss about that paper and share ideas. <br>
<br>
**[About Season 1]** <br>
Yonsei NLP Study Season 1 (2021.07.07 ~ 2021.08.25) <br>
Main Topic : Various Pretraining Methods of Language Models in NLP / Transformer based Models <br>
During this period we have covered 37 papers in total! <br>
<br>
**[About this repository]** <br>
This repository contains presentation materials, links to presentation videos, and a summary of all the papers we have studied in Yonsei NLP Study Season1(2021.07.07~2021.08.25). <br>
<br>
## About the Members
* [Seungone Kim(김승원)](https://github.com/SeungoneKim)
* [GuiJin Son(손규진)](https://github.com/ampehta)
* [Hyungjoo Chae(채형주)](https://github.com/kyle8581)
* [Sejun Joo(주세준)](https://github.com/joocjun)



## List of Papers we covered
```
1. Universal Language Model Fine-tuning for Text Classification (https://arxiv.org/abs/1801.06146)
2. Deep contextualized word representations (https://arxiv.org/abs/1802.05365)
3. Attention is All You Need (https://arxiv.org/abs/1706.03762)
4. BERT: Pre-training of Deep bidirectional Transformers for Language Understanding (https://arxiv.org/abs/1810.04805)
5. Improving Language Understanding by Generative Pre-Training 


```
